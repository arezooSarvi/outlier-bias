{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from data_util import *\n",
    "import metrics\n",
    "import time\n",
    "import pandas as pd\n",
    "from MyUtil import *\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "graded_datapath = {\n",
    "'yahoo':'_data/ltrc_yahoo/set1.binarized_purged_querynorm_filtered.npz',\n",
    "'mslr':'_data/MSLR-WEB30k/Fold1/binarized_purged_querynorm_filtered.npz',\n",
    "               }\n",
    "binary_datapath = {\n",
    "'yahoo':'_data/ltrc_yahoo/set1.binarized_purged_querynorm_filtered.binrel.pkl',\n",
    "'mslr':'_data/MSLR-WEB30k/Fold1/binarized_purged_querynorm_filtered.binrel.pkl',\n",
    "               }\n",
    "\n",
    "results_dir = '_data/outlier/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import trange\n",
    "\n",
    "\n",
    "def get_params(params, outliers=9, topk=20):\n",
    "    if isinstance(params, str):\n",
    "        with open(params, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "    params = pd.DataFrame(params).T\n",
    "    sessions_dist = params['count'][:outliers+1]\n",
    "    sessions_dist /= sessions_dist.sum()\n",
    "    sessions_dist = sessions_dist.values\n",
    "    for col in ['propensity', 'epsilon_p', 'epsilon_n']:\n",
    "        params[col] = params[col].apply(lambda x: np.array(x).astype(np.float)[None,:topk])\n",
    "    params['propensity'] = params['propensity'].apply(np.array)\n",
    "    p = np.concatenate(params['propensity'][:outliers+1].values, 0)\n",
    "    ep = np.concatenate(params['epsilon_p'][:outliers+1].values, 0)\n",
    "    en = np.concatenate(params['epsilon_n'][:outliers+1].values, 0)\n",
    "    betas = p * en\n",
    "    alphas = p * ep - betas\n",
    "    return alphas, betas, sessions_dist.astype(np.float)\n",
    "    \n",
    "def make_params_readable(pkl_path):\n",
    "    if isinstance(pkl_path, str):\n",
    "        with open(pkl_path, 'rb') as f:\n",
    "            params = pd.DataFrame(pickle.load(f))\n",
    "    else:\n",
    "        params = pkl_path\n",
    "    df = pd.DataFrame(params).T\n",
    "    topk = len(df.propensity.iloc[0])\n",
    "    for col in ['propensity', 'epsilon_n', 'epsilon_p']:\n",
    "        df[[f'{col}_{i+1}' for i in range(topk)]] = df[col].to_list()\n",
    "    for i in range(topk):\n",
    "        df[f'zp_{i+1}'] = df[f'propensity_{i+1}'] * df[f'epsilon_p_{i+1}']\n",
    "        df[f'zn_{i+1}'] = df[f'propensity_{i+1}'] * df[f'epsilon_n_{i+1}']\n",
    "        df[f'z_{i+1}'] = df[[f'zp_{i+1}', f'zn_{i+1}']].agg(list, axis=1)\n",
    "    return df[[f'z_{i+1}' for i in range(topk)]]\n",
    "\n",
    "def generate_clicks_pbm(params, outliers_max_pos, topk, ds, sessions_cnt, save_path, outliers_path):\n",
    "    alphas, betas, sessions_dist = get_params(params, outliers=outliers_max_pos, topk=topk)\n",
    "      \n",
    "    clicks = []\n",
    "    session_inds = np.random.choice(\n",
    "        np.arange(alphas.shape[0]), \n",
    "        size=(ds.trdlr.shape[0] - 1,), replace=True, p=sessions_dist)\n",
    "    total_clicks_cnt = 0\n",
    "    outlierness = []\n",
    "    \n",
    "#     Make the labels binary if not already!\n",
    "    if ds.trlv.max() > 1:\n",
    "        ds.trlv = np.round(ds.trlv/ds.trlv.max(), 0)\n",
    "    \n",
    "    for qid in trange(ds.trdlr.shape[0] - 1, leave=False):\n",
    "        s, e = ds.trdlr[qid: qid+2]\n",
    "        session_ind = min(session_inds[qid], e-s)\n",
    "        alpha = alphas[session_ind]\n",
    "        beta = betas[session_ind]\n",
    "        q_session = np.repeat(ds.sessions[qid], sessions_cnt//ds.sessions[qid].shape[0], 0)\n",
    "        rel = ds.trlv[s:e]\n",
    "        q_a = np.ones_like(rel) * alpha[-1]\n",
    "        q_a[:min(e-s, alpha.shape[0])] = alpha[:min(e-s, alpha.shape[0])]\n",
    "        q_b = np.ones_like(rel) * beta[-1]\n",
    "        q_b[:min(e-s, beta.shape[0])] = beta[:min(e-s, beta.shape[0])]\n",
    "        c = q_a * rel[q_session] + q_b\n",
    "        clicks.append(np.random.binomial(1, c))\n",
    "        total_clicks_cnt += len(np.where(clicks[-1]==1)[0])\n",
    "        q_outliers = np.zeros(e-s)\n",
    "        if session_ind > 0:\n",
    "            q_outliers[session_ind - 1] = 1.\n",
    "        outlierness.append(q_outliers)\n",
    "        \n",
    "    with open(save_path, 'wb') as f:\n",
    "        pickle.dump(clicks, f)\n",
    "    with open(outliers_path, 'wb') as f:\n",
    "        pickle.dump(np.concatenate(outlierness,0), f)\n",
    "    return total_clicks_cnt\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Binarize levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_name in ['yahoo', 'mslr']:\n",
    "    dataset = read_pkl(graded_datapath[dataset_name])\n",
    "    dataset.trlv = np.round(dataset.trlv/4,0)\n",
    "    dataset.valv = np.round(dataset.valv/4,0)\n",
    "    dataset.telv = np.round(dataset.telv/4,0)\n",
    "    with open(binary_datapath[dataset_name], 'wb') as f:\n",
    "        pickle.dump(dataset.__dict__, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Production ranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_datapath = {\n",
    "    \"yahoo\": \"_data/ltrc_yahoo/sessions_fix.pkl\",\n",
    "    \"mslr\": \"_data/MSLR-WEB30k/Fold1/sessions_fix.pkl\"\n",
    "}\n",
    "\n",
    "\n",
    "def lambdarank(dataset, model_path=None, learning_rate=0.05, num_leaves=31, n_estimators=300, eval_at=[10], early_stopping_rounds=10000):\n",
    "    start = time.time()\n",
    "    if model_path is not None and os.path.exists(model_path):\n",
    "        booster = lgb.Booster(model_file=model_path)\n",
    "        print('loading lgb took {} secs.'.format(time.time() - start))\n",
    "        return booster\n",
    "\n",
    "    gbm = lgb.LGBMRanker(learning_rate=learning_rate, n_estimators=n_estimators, num_leaves=num_leaves)\n",
    "\n",
    "    gbm.fit(dataset.trfm, dataset.trlv, \n",
    "          group=np.diff(dataset.trdlr), \n",
    "          eval_set=[(dataset.vafm, dataset.valv)],\n",
    "          eval_group=[np.diff(dataset.vadlr)], \n",
    "          eval_at=eval_at, \n",
    "          early_stopping_rounds=early_stopping_rounds, \n",
    "          verbose=False)\n",
    "\n",
    "    if model_path is not None:\n",
    "        gbm.booster_.save_model(model_path)\n",
    "\n",
    "    print('training lgb took {} secs.'.format(time.time() - start))\n",
    "    return gbm.booster_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for dataset_name in ['yahoo', 'mslr']:\n",
    "    with open(binary_datapath[dataset_name], 'rb') as f:\n",
    "        dataset = type('ltr', (object,), pickle.load(f))\n",
    "    small_dataset = subsample_splits(dataset, 20, 777)\n",
    "    booster = lambdarank(small_dataset, model_path=f'{dataset_name}_production.gbt')\n",
    "    \n",
    "    te_y_pred = booster.predict(dataset.tefm)\n",
    "    te_metric = metrics.LTRMetrics(dataset.telv,np.diff(dataset.tedlr),te_y_pred)\n",
    "    tr_y_pred = booster.predict(dataset.trfm)\n",
    "    tr_metric = metrics.LTRMetrics(dataset.trlv,np.diff(dataset.trdlr),tr_y_pred)\n",
    "    \n",
    "    print(dataset_name, 'train:', tr_metric.NDCG(10), ', test:', te_metric.NDCG(10))\n",
    "    \n",
    "    sessions = []\n",
    "    for qid in range(dataset.trdlr.shape[0] - 1):\n",
    "        s, e = dataset.trdlr[qid: qid+2]\n",
    "        y = tr_y_pred[s:e]\n",
    "        argsorted = y.argsort()[::-1]\n",
    "        \n",
    "        sessions.append(argsorted[None,:])\n",
    "        \n",
    "        with open(sessions_datapath[dataset_name], 'wb') as f:\n",
    "            pickle.dump(sessions, f)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### top10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk = 10\n",
    "\n",
    "topk_datapath = {\n",
    "'yahoo': f'_data/ltrc_yahoo/top{topk}.set1.binarized_purged_querynorm_filtered.binrel.pkl',\n",
    "'mslr': f'_data/MSLR-WEB30k/Fold1/top{topk}.binarized_purged_querynorm_filtered.binrel.pkl',\n",
    "               }\n",
    "\n",
    "topk_sessions_datapath = {\n",
    "    \"yahoo\": f\"_data/ltrc_yahoo/top{topk}.sessions_fix.pkl\",\n",
    "    \"mslr\": f\"_data/MSLR-WEB30k/Fold1/top{topk}.sessions_fix.pkl\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for dataset_name in ['yahoo', 'mslr']:\n",
    "    with open(sessions_datapath[dataset_name], 'rb') as f:\n",
    "        sessions = pickle.load(f)\n",
    "    with open(binary_datapath[dataset_name], 'rb') as f:\n",
    "        dataset = pickle.load(f)\n",
    "    \n",
    "    topk_sessions = []\n",
    "    fms, dlrs, lvs = [], [0], []\n",
    "    for qid in range(len(sessions)):\n",
    "        session = sessions[qid][0,:topk]\n",
    "        s_i, e_i = dataset['trdlr'][qid:qid+2]\n",
    "        fm_ = dataset['trfm'][s_i:e_i, :]\n",
    "        lv_ = dataset['trlv'][s_i:e_i]\n",
    "        if sum(lv_[session]) > 1:\n",
    "            fms.append(fm_[session, :])\n",
    "            dlrs.append(session.shape[0])\n",
    "            lvs.append(lv_[session])\n",
    "            topk_sessions.append(np.arange(session.shape[0])[None,:])\n",
    "    dataset['trfm'] = np.concatenate(fms, 0)\n",
    "    dataset['trdlr'] = np.cumsum(dlrs)\n",
    "    dataset['trlv'] = np.concatenate(lvs, 0)\n",
    "    \n",
    "    \n",
    "    print(f'after top{topk}:', dataset_name, metrics.LTRMetrics(dataset['trlv'],np.diff(dataset['trdlr']),-np.arange(dataset['trlv'].shape[0])).NDCG(10))\n",
    "\n",
    "    with open(topk_sessions_datapath[dataset_name], 'wb') as f:\n",
    "        pickle.dump(topk_sessions, f)\n",
    "    \n",
    "    with open(topk_datapath[dataset_name], 'wb') as f:\n",
    "        pickle.dump(dataset, f)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### relevant freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "for dataset_name in ['yahoo', 'mslr']:\n",
    "    with open(topk_datapath[dataset_name], 'rb') as f:\n",
    "        dataset = pickle.load(f)\n",
    "        \n",
    "    freq = defaultdict(lambda : 0)\n",
    "    print(dataset_name, dataset['trdlr'].shape[0] - 1, 'queries')\n",
    "    for qid in range(dataset['trdlr'].shape[0] - 1):\n",
    "        s_i , e_i = dataset['trdlr'][qid:qid+2]\n",
    "        freq[sum(dataset['trlv'][s_i:e_i])] += 1\n",
    "    \n",
    "    print(dict(freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Full info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_results(jobid, results_dir = '_data/outlier/'):\n",
    "    \n",
    "    files_list = os.listdir(results_dir)\n",
    "\n",
    "    dfs = []\n",
    "    for file in files_list:\n",
    "        if ((jobid is not None and jobid in file) or (jobid is None)) and file.endswith('.json'):\n",
    "            df = pd.read_json(os.path.join(results_dir, file), lines=True)\n",
    "            df['file_name'] = file\n",
    "            dfs.append(df)\n",
    "    full_df = pd.concat(dfs)\n",
    "\n",
    "    max_epoch_dfs = []\n",
    "    for dataset in full_df.dataset.unique():\n",
    "        tmp_df = full_df.loc[(full_df.dataset == dataset)]\n",
    "        max_epoch_dfs.append(tmp_df.loc[(tmp_df.epoch == tmp_df.epoch.max())])\n",
    "        print(dataset, 'epoch:', tmp_df.epoch.max())\n",
    "    full_df = pd.concat(max_epoch_dfs)\n",
    "    values=['train_clicks','train']\n",
    "    if 'valid' in full_df:\n",
    "        values.append('valid')\n",
    "    if 'test' in full_df:\n",
    "        values.append('test')\n",
    "        full_df = full_df.loc[~full_df.test.isna()]\n",
    "    pv = pd.pivot_table(full_df, values=values, columns=['dataset'], aggfunc=lambda x: f'{len(x)} -> {np.nanmean(x):.04f} ' + u\"\\u00B1\" + f' {np.nanstd(x):.04f}')\n",
    "    return pv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_dists(big, small, position, alpha):\n",
    "    mid_pos = int(len(small)/2)\n",
    "    if position < mid_pos:\n",
    "        small = small[mid_pos-position:]\n",
    "    small = np.pad(small, (max(0, position-mid_pos), 0,))\n",
    "    return (1-alpha) * big + (alpha) * np.pad(small, (0, len(big)-len(small),))\n",
    "\n",
    "def put_on_top(big, small, position, alpha):\n",
    "    mid_pos = int(len(small)/2)\n",
    "    if position < mid_pos:\n",
    "        small = small[mid_pos-position:]\n",
    "    start_pos = max(0, position-mid_pos)\n",
    "    big[start_pos:start_pos + len(small)] = (1-alpha) * big[start_pos:start_pos + len(small)] + (alpha) * small\n",
    "    return big\n",
    "    \n",
    "def create_params(outlier_share, outliers=7, topk=20, only_outliers=True):\n",
    "    theta = np.array([(1./(i+1.)) for i in range(topk)])\n",
    "    ep = np.array([0.98-(i/100.) for i in range(topk)])\n",
    "    en = np.ones_like(theta)*0.05\n",
    "\n",
    "    gauss_theta = np.array([0.3, 1, 0.3, 0.05])\n",
    "    gaussp = np.ones_like(gauss_theta)\n",
    "    gaussn = np.ones_like(gauss_theta)*0.05\n",
    "    \n",
    "    params = {}\n",
    "    offset = 0.\n",
    "    if not only_outliers:\n",
    "        params['0.0'] = {'propensity':list(theta), 'epsilon_p':list(ep), 'epsilon_n':list(en), 'count':100}\n",
    "        offset = 1.\n",
    "    for position in range(outliers):\n",
    "        params[f'{position + offset}'] = {'propensity':list(add_dists(theta, gauss_theta, position, outlier_share)), \n",
    "                                       'epsilon_p':list(put_on_top(ep, gaussp, position, outlier_share)), \n",
    "                                       'epsilon_n':list(put_on_top(en, gaussn, position, outlier_share)),\n",
    "                                       'count':50}\n",
    "        \n",
    "    return params\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**PBM clicks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### simple pbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = create_params(outlier_share=0, outliers=0, topk=20, only_outliers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks_path = {'yahoo': '_data/ltrc_yahoo/top20.clicks',\n",
    "              'mslr': '_data/MSLR-WEB30k/Fold1/top20.clicks'}\n",
    "\n",
    "for dataset_name in ['yahoo', 'mslr']:\n",
    "    ds = load_dataset(dataset_name, 'datasets_info.json', 400)\n",
    "    params_real_world = f'simple'\n",
    "\n",
    "    click_cnt = generate_clicks_pbm(params = params, outliers_max_pos = 0, topk = 20, \n",
    "                                    ds = ds, sessions_cnt = 400, \n",
    "                                    save_path = f'{clicks_path[dataset_name]}_{params_real_world}.pkl', \n",
    "                                    outliers_path = f'{clicks_path[dataset_name]}_{params_real_world}.pkl'.replace('clicks', 'outlierness')\n",
    "                                   )\n",
    "    print(dataset_name, '->', click_cnt, 'clicks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OPBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks_path = {'yahoo': '_data/ltrc_yahoo/top10.clicks',\n",
    "              'mslr': '_data/MSLR-WEB30k/Fold1/top10.clicks'}\n",
    "\n",
    "for dataset_name in ['yahoo', 'mslr']:\n",
    "    ds = load_dataset(dataset_name, 'datasets_info.json', 400)\n",
    "    for outlier_share in [0.25, 0.5, 0.6, 0.7, 0.75, 0.8, 0.85, 0.9]:\n",
    "        params = create_params(outlier_share=outlier_share, outliers=9, topk=10, only_outliers=True)\n",
    "        params_real_world = f'opbm_{outlier_share:.02f}'\n",
    "\n",
    "        click_cnt = generate_clicks_pbm(params = params, outliers_max_pos = 9, topk = 10, \n",
    "                                        ds = ds, sessions_cnt = 400, \n",
    "                                        save_path = f'{clicks_path[dataset_name]}_{params_real_world}.pkl', \n",
    "                                        outliers_path = f'{clicks_path[dataset_name]}_{params_real_world}.pkl'.replace('clicks', 'outlierness')\n",
    "                                       )\n",
    "        print(dataset_name, outlier_share, '->', click_cnt, 'clicks')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = create_params(outlier_share=0.75, outliers=9, topk=10, only_outliers=True)\n",
    "params['9.0'] = {}\n",
    "for key in params['8.0']:\n",
    "    if isinstance(params['8.0'][key], list):\n",
    "        params['9.0'][key] = list(0.5*np.array(params['3.0'][key]) + 0.5*np.array(params['8.0'][key]))\n",
    "    else:\n",
    "        params['9.0'][key] = params['8.0'][key]\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_clicks_pbm_mix_lazy(params, outliers_max_pos, topk, ds, sessions_cnt, save_path, outliers_path):\n",
    "    alphas, betas, sessions_dist = get_params(params, outliers=outliers_max_pos, topk=topk)\n",
    "      \n",
    "    print(alphas.shape[0])\n",
    "    clicks = []\n",
    "    session_inds = np.random.choice(\n",
    "        np.arange(alphas.shape[0]), \n",
    "        size=(ds.trdlr.shape[0] - 1,), replace=True, p=sessions_dist)\n",
    "    total_clicks_cnt = 0\n",
    "    outlierness = []\n",
    "    outlierness_lazy = []\n",
    "    \n",
    "#     Make the labels binary if not already!\n",
    "    if ds.trlv.max() > 1:\n",
    "        ds.trlv = np.round(ds.trlv/ds.trlv.max(), 0)\n",
    "    \n",
    "    print(ds.trdlr.shape, ds.trdlr[-1])\n",
    "    for qid in trange(ds.trdlr.shape[0] - 1, leave=False):\n",
    "        s, e = ds.trdlr[qid: qid+2]\n",
    "        session_ind = min(session_inds[qid], e-s)\n",
    "        alpha = alphas[session_ind]\n",
    "        beta = betas[session_ind]\n",
    "        q_session = np.repeat(ds.sessions[qid], sessions_cnt//ds.sessions[qid].shape[0], 0)\n",
    "        rel = ds.trlv[s:e]\n",
    "        q_a = np.ones_like(rel) * alpha[-1]\n",
    "        q_a[:min(e-s, alpha.shape[0])] = alpha[:min(e-s, alpha.shape[0])]\n",
    "        q_b = np.ones_like(rel) * beta[-1]\n",
    "        q_b[:min(e-s, beta.shape[0])] = beta[:min(e-s, beta.shape[0])]\n",
    "        c = q_a * rel[q_session] + q_b\n",
    "        clicks.append(np.random.binomial(1, c))\n",
    "        total_clicks_cnt += len(np.where(clicks[-1]==1)[0])\n",
    "        q_outliers = np.zeros(e-s)\n",
    "        q_outliers_lazy = np.zeros(e-s)\n",
    "        if session_ind > 0:\n",
    "            q_outliers[session_ind - 1] = 1.\n",
    "            if session_ind == 9:\n",
    "                session_ind = 3\n",
    "            q_outliers_lazy[session_ind - 1] = 1.\n",
    "                \n",
    "        outlierness.append(q_outliers)\n",
    "        outlierness_lazy.append(q_outliers_lazy)\n",
    "        \n",
    "    print(len(outlierness))\n",
    "    print(np.concatenate(outlierness,0).shape)\n",
    "    with open(save_path, 'wb') as f:\n",
    "        pickle.dump(clicks, f)\n",
    "    with open(outliers_path, 'wb') as f:\n",
    "        pickle.dump(np.concatenate(outlierness,0), f)\n",
    "    with open(save_path.replace('.pkl', '_lazy.pkl'), 'wb') as f:\n",
    "        pickle.dump(clicks, f)\n",
    "    with open(outliers_path.replace('.pkl', '_lazy.pkl'), 'wb') as f:\n",
    "        pickle.dump(np.concatenate(outlierness_lazy,0), f)\n",
    "    return total_clicks_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks_path = {'yahoo': '_data/ltrc_yahoo/top10.clicks',\n",
    "              'mslr': '_data/MSLR-WEB30k/Fold1/top10.clicks'}\n",
    "\n",
    "dataset_name = 'mslr'\n",
    "ds = load_dataset(dataset_name, 'datasets_info.json', 400)\n",
    "params = create_params(outlier_share=0.8, outliers=9, topk=10, only_outliers=True)\n",
    "params['9.0'] = {}\n",
    "for key in params['8.0']:\n",
    "    if isinstance(params['8.0'][key], list):\n",
    "        params['9.0'][key] = list(0.5*np.array(params['3.0'][key]) + 0.5*np.array(params['8.0'][key]))\n",
    "    else:\n",
    "        params['9.0'][key] = params['8.0'][key]\n",
    "\n",
    "for count in [3, 6, 12, 25, 50, 100]:\n",
    "    params['9.0']['count'] = count\n",
    "    params_real_world = f'opbm_0.8_mix_{count}'\n",
    "\n",
    "    click_cnt = generate_clicks_pbm_mix_lazy(params = params, outliers_max_pos = 10, topk = 10, \n",
    "                                    ds = ds, sessions_cnt = 400, \n",
    "                                    save_path = f'{clicks_path[dataset_name]}_{params_real_world}.pkl', \n",
    "                                    outliers_path = f'{clicks_path[dataset_name]}_{params_real_world}.pkl'.replace('clicks', 'outlierness')\n",
    "                                   )\n",
    "    print(dataset_name, '->', click_cnt, 'clicks')\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
